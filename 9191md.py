import requests
from urllib.parse import urlparse
import os

def get_domain(url):
    """æå–ä¸»åŸŸåä½œä¸ºæ–‡ä»¶å"""
    return urlparse(url).netloc

def fetch_all_pages(base_url, base_params):
    """åˆ†é¡µæŠ“å–æ‰€æœ‰è§†é¢‘å¹¶æŒ‰ç±»å‹åˆ†ç±»"""
    page = 1
    result = {}
    total_pages = None

    while True:
        params = base_params.copy()
        params["pg"] = page

        try:
            response = requests.get(base_url, params=params, timeout=10)
            data = response.json()
        except Exception as e:
            print(f"[é”™è¯¯] ç¬¬ {page} é¡µè¯·æ±‚å¤±è´¥ï¼š{e}")
            break

        if data.get("code") != 1 or "list" not in data:
            print(f"[è­¦å‘Š] ç¬¬ {page} é¡µè¿”å›æ— æ•ˆæ•°æ®ï¼Œåœæ­¢é‡‡é›†")
            break

        if total_pages is None:
            total_pages = data.get("pagecount", 1)
            print(f"æ€»é¡µæ•°ï¼š{total_pages}")

        print(f"â†’ æ­£åœ¨é‡‡é›†ç¬¬ {page}/{total_pages} é¡µï¼Œå…± {len(data['list'])} é¡¹")

        for item in data["list"]:
            type_name = item.get("type_name", "æœªçŸ¥åˆ†ç±»")
            vod_name = item.get("vod_name", "æœªå‘½å")
            vod_play_url = item.get("vod_play_url", "")

            entries = []
            for part in vod_play_url.split("#"):
                if "$" in part:
                    name, url = part.split("$", 1)
                else:
                    name, url = vod_name, part
                entries.append(f"{name}, {url}")

            result.setdefault(type_name, []).extend(entries)

        if page >= total_pages:
            break
        page += 1

    print("åˆ†ç±»æ±‡æ€»ï¼š")
    for cat, items in result.items():
        print(f"  â€¢ {cat}: {len(items)} æ¡")

    return result

def save_grouped_to_file(grouped_data, filename):
    """ä¿å­˜ä¸ºæ–‡æœ¬æ–‡ä»¶ï¼Œåˆ†ç±»æ•´ç†"""
    with open(filename, "w", encoding="utf-8") as f:
        for type_name, items in grouped_data.items():
            f.write(f"{type_name}, #genre#\n")
            for line in items:
                f.write(f"{line}\n")
            f.write("\n")
    print(f"å·²ä¿å­˜åˆ°æ–‡ä»¶ï¼š{filename}")

def main():
    # ğŸ”§ ä½ çš„æºåœ°å€å’Œå‚æ•°é…ç½®åœ¨è¿™é‡Œ
    base_url = "http://www.9191md.me/api.php/provide/vod/"
    base_params = {
        "ac": "list",
        "type": "",  # å¯æŒ‡å®šç±»å‹ IDï¼Œä¸å¡«ä¸ºå…¨éƒ¨
        "pg": 1
    }

    print(f"å¼€å§‹é‡‡é›†ï¼š{base_url}")
    grouped_data = fetch_all_pages(base_url, base_params)
    domain = get_domain(base_url)
    filename = f"{domain}.txt"
    save_grouped_to_file(grouped_data, filename)

if __name__ == "__main__":
    main()
